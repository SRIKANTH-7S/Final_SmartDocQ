# smartinterview.py
import os
import re
import json
import fitz
import docx
import numpy as np
import faiss
from typing import List, Optional, Tuple
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import logging
from dotenv import load_dotenv

# -----------------------------
# Environment Setup
# -----------------------------
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    raise ValueError("GOOGLE_API_KEY not set in environment")

genai.configure(api_key=API_KEY)
EMBED_MODEL_NAME = "all-MiniLM-L6-v2"
_embed_model = SentenceTransformer(EMBED_MODEL_NAME)


# -----------------------------
# Document Utilities
# -----------------------------
def read_pdf(path: str) -> str:
    """Reads a PDF file and returns its text."""
    doc = fitz.open(path)
    return "\n".join([p.get_text() for p in doc])

def read_docx(path: str) -> str:
    """Reads a DOCX file and returns its text."""
    d = docx.Document(path)
    return "\n".join([p.text for p in d.paragraphs])

def read_txt(path: str) -> str:
    """Reads a plain text file and returns its content."""
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def read_document_file(path: str) -> str:
    """Reads any supported document type and returns text."""
    ext = os.path.splitext(path)[1].lower()
    if ext == ".pdf": return read_pdf(path)
    if ext == ".docx": return read_docx(path)
    if ext == ".txt": return read_txt(path)
    raise ValueError(f"Unsupported file type: {ext}")


# -----------------------------
# Text Chunking and Embeddings
# -----------------------------
def chunk_text(text: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:
    """Splits text into overlapping chunks for semantic embedding."""
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        j = min(i + chunk_size, len(words))
        chunk = " ".join(words[i:j]).strip()
        if chunk:
            chunks.append(chunk)
        i += chunk_size - overlap
    return chunks

def get_embeddings(texts: List[str]) -> np.ndarray:
    """Generates embeddings for a list of texts."""
    arr = _embed_model.encode(texts, show_progress_bar=False)
    if isinstance(arr, list):
        arr = np.array(arr)
    return np.asarray(arr).astype("float32")

def build_faiss_index(chunks: List[str]) -> Tuple[faiss.IndexFlatL2, List[str]]:
    """Builds a FAISS index from a list of text chunks."""
    embeddings = get_embeddings(chunks)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index, chunks


# -----------------------------
# Gemini Helper
# -----------------------------
def gemini_generate(prompt: str) -> str:
    """Uses Google Gemini to generate a text response."""
    try:
        model = genai.GenerativeModel("gemini-2.0-flash")
        res = model.generate_content(prompt)
        # Log for debugging (avoid logging prompt with secrets)
        logging.debug("gemini_generate: received response object type: %s", type(res))
        if hasattr(res, "text") and res.text:
            logging.debug("gemini_generate: text length=%d", len(res.text))
            return res.text.strip()
        elif hasattr(res, "candidates") and res.candidates:
            # candidates can be a list of candidate objects
            first = res.candidates[0]
            content = getattr(first, "content", None) or getattr(first, "text", None) or str(first)
            logging.debug("gemini_generate: candidate content length=%d", len(str(content)))
            return str(content).strip()
        else:
            logging.debug("gemini_generate: unexpected response, returning str(res)")
            return str(res)
    except Exception as e:
        logging.exception("gemini_generate error")
        return f"Gemini error: {repr(e)}"


# -----------------------------
# Interview Copilot
# -----------------------------
class InterviewCopilot:
    """
    AI Interview assistant that:
    1. Loads a document and creates embeddings
    2. Generates interview questions from context
    3. Evaluates user answers and gives feedback
    """

    def __init__(self):
        self.index: Optional[faiss.IndexFlatL2] = None
        self.chunks: Optional[List[str]] = None
        self.questions: List[str] = []
        self.feedback: List[dict] = []
        self.structured_questions: Optional[List[dict]] = None

    # -------------------------
    # Load Document
    # -------------------------
    def load_document(self, file_path: str) -> str:
        """Loads and indexes the given document."""
        text = read_document_file(file_path)
        chunks = chunk_text(text, chunk_size=300, overlap=50)
        self.index, self.chunks = build_faiss_index(chunks)
        return f"âœ… Document loaded successfully. {len(self.chunks)} chunks indexed."

    # -------------------------
    # Question Generation
    # -------------------------
    def generate_questions(self, num_questions: int = 5, level: str = "medium", qtype: Optional[str] = None, **kwargs) -> List[str]:
        """Generates interview questions from document content."""
        if not self.chunks:
            raise ValueError("Please load a document first.")
        # Support legacy 'qtype' parameter by mapping it to level if provided
        if (level is None or str(level).strip() == "") and qtype:
            level = qtype

        context = "\n\n".join(self.chunks[:6])

        # If a specific qtype is requested, ask the model for a strict JSON array of questions
        if qtype:
            qtype_lower = qtype.lower()
            # build type-specific JSON schema and instruction
            if qtype_lower == "mcq":
                instr = (
                    "Return a JSON array with exactly {n} MCQ objects based ONLY on the Document Context. "
                    "Each object must have: question (string), options (array of 4 option strings labeled 'A) ...', 'B) ...'), and correct (the correct option label and text, e.g. 'B) Option text'). "
                )
            elif qtype_lower == "hr":
                instr = (
                    "Return a JSON array with exactly {n} HR (behavioral) question objects. "
                    "Each object must have: question (string) and if helpful a short scenario field."
                )
            elif qtype_lower == "technical":
                instr = (
                    "Return a JSON array with exactly {n} Technical question objects. "
                    "Each object must have: question (string) and difficulty ('easy'|'medium'|'hard')."
                )
            else:
                instr = (
                    "Return a JSON array with exactly {n} question objects suitable for the requested type. "
                    "Each object must have: question (string)."
                )

            prompt = f"""
You are an interview question generator. Follow these rules strictly.

Document Context:
{context}

{instr}

Return ONLY valid JSON (no surrounding text). The JSON must be an array with exactly {num_questions} elements. Example for MCQ:
[{{"question":"...","options":["A) ...","B) ...","C) ...","D) ..."],"correct":"B) ..."}}, ...]
""".replace("{n}", str(num_questions)).replace("{instr}", instr)

            result = gemini_generate(prompt)

            # Try to extract JSON from the model output
            try:
                match = re.search(r"\[\s*\{.*\}\s*\]", result, re.S)
                json_text = match.group(0) if match else result
                parsed = json.loads(json_text)
                if isinstance(parsed, list) and len(parsed) >= 1:
                    structured = parsed[:num_questions]
                    # Basic validation/cleanup for MCQ
                    if qtype_lower == "mcq":
                        validated = []
                        for item in structured:
                            q = {
                                "question": str(item.get("question", "")).strip(),
                                "options": item.get("options") if isinstance(item.get("options"), list) else [],
                                "correct": item.get("correct", "")
                            }
                            # Ensure labels for options
                            if len(q["options"]) == 4:
                                validated.append(q)
                        if len(validated) >= num_questions:
                            self.structured_questions = validated[:num_questions]
                            self.questions = [s["question"] for s in self.structured_questions]
                            return self.questions
                    else:
                        # For other types, accept parsed questions if they have 'question'
                        validated = [ {"question": str(it.get("question", "")).strip(), **({k:v for k,v in it.items() if k!='question'} )} for it in structured if it.get("question") ]
                        if len(validated) >= num_questions:
                            self.structured_questions = validated[:num_questions]
                            self.questions = [s["question"] for s in self.structured_questions]
                            return self.questions
            except Exception:
                logging.debug("generate_questions: structured JSON parse failed", exc_info=True)

            # If structured parse fails, continue to free-form extraction below
        else:
            # No qtype requested: use the previous free-form prompt
            prompt = f"""
You are an interview question generator.
Generate exactly {num_questions} {level}-level interview questions based ONLY on the following document context.

Rules:
- No intro text.
- Each line = one valid question ending with a question mark (?).

Document Context:
{context}

Return only the list of questions, one per line.
"""
            result = gemini_generate(prompt)

        # Log raw output for debugging
        logging.info("generate_questions: gemini result length=%d", len(result) if isinstance(result, str) else 0)
        logging.debug("generate_questions: raw result:\n%s", result)

        # Attempt multiple extraction strategies to be robust against different response formats
        clean_qs: List[str] = []

        # 1) Extract numbered lines that end with a question mark, e.g. '1) What ...?'
        try:
            numbered = re.findall(r"(?m)^\s*\d+\s*[\).:-]*\s*(.+?\?)\s*$", result)
            if numbered:
                clean_qs.extend([q.strip() for q in numbered])
        except Exception:
            logging.debug("generate_questions: numbered extraction failed", exc_info=True)

        # 2) Extract any sentence that ends with a question mark
        try:
            ques = re.findall(r"([^\n\r?.!]+\?)", result)
            if ques:
                clean_qs.extend([q.strip() for q in ques if q.strip()])
        except Exception:
            logging.debug("generate_questions: question mark extraction failed", exc_info=True)

        # 3) Fallback: split lines and keep those that look like questions or start with question words
        if len(clean_qs) < num_questions:
            raw_lines = [line.strip(" -0123456789.)\t\u2022") for line in result.splitlines() if line.strip()]
            question_keywords = ("what", "why", "how", "when", "where", "name", "explain", "describe", "list", "define", "which", "who")
            for line in raw_lines:
                l = line.strip()
                if not l:
                    continue
                if l.endswith("?") or l.lower().startswith(question_keywords):
                    clean_qs.append(l)

        # Deduplicate while preserving order
        seen = set()
        deduped: List[str] = []
        for q in clean_qs:
            if q not in seen:
                deduped.append(q)
                seen.add(q)

        # If still not enough, create simple fallback questions from document chunks
        if len(deduped) < num_questions:
            logging.info("generate_questions: insufficient questions from Gemini (%d), creating fallback questions", len(deduped))
            for i, chunk in enumerate(self.chunks[:num_questions * 2]):
                if len(deduped) >= num_questions:
                    break
                snippet = chunk.split(".")[0][:120].strip()
                if not snippet:
                    continue
                fallback_q = f"Explain the following: {snippet}?"
                if fallback_q not in deduped:
                    deduped.append(fallback_q)

        self.questions = deduped[:num_questions]
        return self.questions

    # -------------------------
    # Answer Evaluation
    # -------------------------
    def evaluate_answers(self, user_answers: List[str]) -> Tuple[float, List[dict]]:
        """Evaluates answers using Gemini and returns average score + detailed feedback."""
        if not self.questions:
            raise ValueError("No questions have been generated.")
        if len(user_answers) != len(self.questions):
            raise ValueError("Number of answers must match number of questions.")

        total_score = 0
        feedback_list = []

        for q, a in zip(self.questions, user_answers):
            context = "\n\n".join(self.chunks[:5])
            prompt = f"""
You are an interviewer evaluating a candidate's response.

Question: {q}
Candidate Answer: {a}

Using the context below:
{context}

Evaluate the answer and give:
- score: integer from 0 to 10
- short feedback: 1-2 sentences explaining strengths/weaknesses
- correct_answer: the ideal or model answer (a concise phrase or sentence). For MCQs return the correct option letter and text (e.g. 'B) Option text').

Return JSON exactly (no extra text), for example:
{{"score": 7, "feedback": "Good explanation...", "correct_answer": "B) Option text"}}
"""
            eval_text = gemini_generate(prompt)

            try:
                data = json.loads(re.search(r'\{.*\}', eval_text, re.S).group())
            except Exception:
                logging.debug("evaluate_answers: failed to parse eval_text: %s", eval_text)
                data = {"score": 0, "feedback": "âš  Could not parse Gemini response.", "correct_answer": ""}

            feedback_list.append({
                "question": q,
                "user_answer": a,
                "score": data.get("score", 0),
                "feedback": data.get("feedback", ""),
                "correct_answer": data.get("correct_answer", "")
            })
            total_score += data.get("score", 0)

        avg_score = round(total_score / len(self.questions), 2)
        self.feedback = feedback_list
        return avg_score, feedback_list

    # -------------------------
    # Cache Management
    # -------------------------
    def clear_cache(self):
        """Resets loaded data."""
        self.index = None
        self.chunks = None
        self.questions = []
        self.feedback = []
        return "ðŸ§¹ Interview session cleared."


# -----------------------------
# âœ… Example usage (for backend)
# -----------------------------
# from smartinterview import InterviewCopilot
# bot = InterviewCopilot()
# bot.load_document("resume.pdf")
# qs = bot.generate_questions(num_questions=5, level="easy")
# avg, feedback = bot.evaluate_answers(["answer1", "answer2", ...])